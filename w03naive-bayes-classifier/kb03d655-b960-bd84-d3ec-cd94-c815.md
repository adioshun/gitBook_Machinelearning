# 확률 분포 추정 

## 1. 밀도 추정 
모수를 아예 모른다고 가정하고 모집단으로부터 추출된 표본으로부터 모수를 추정하는 방법이다.

![](http://i.imgur.com/D5eLnLK.png)

밀도 추정 \(Density Estimation\)

* 얻어진\(관측된\) 데이터들의 분포로부터 원래 변수의 \(확률\) 분포 특성을 추정하고자 하는 것이 density estimation\(밀도추정\)이다.



밀도 추정 : 
- 데이터로 부터 변수가 가질 수 있는 모든 값의 밀도(확률)을 추정
- 해당 변수에서 관측된 몇가지 '데이터'로부터 변수가 가질 수 있는 모든 값들에 대한 밀도(확률)를 추정하는 것 
- eg.  나무의 색에서 갈색이 나타날 확률은 높은 반면 파란색이 나타날 확률은 낮다는걸 수학적으로 추정하는것이에요.


이처럼 표본을 통해 모집단의 모수를 추정하는 방법에는 크게 세가지가 있다.

1. [method of moments](http://blog.daum.net/gongdjn/51)
2. [method of maximum likelihood](http://blog.daum.net/gongdjn/122)
3. Bayes' method


확률 분포 추정 하는법
![](http://i.imgur.com/3RQirUn.png)

모수적 방법 : 특정한 분포를 따른다고 가정하고 밀도함수의 **파라미터(모수, eg:평균, 분산)**를 추정하는 방법 
- MAP: 최대 사후 확률 
- ML/MLE : 최대 우도 

비모수적 방법 : 어떠한 분포 형태도 가정하지 않고, 직접 밀도 함수를 유도하는 방법 
- 히스토 그램 
- K-NNR
- 파첸의 창 = 커널 밀도 추정(KDE) 



## 1. 모수적 분포 
- 사전 확률 고려 유무

- Parametric 밀도추정
    - 미리 pdf(probability density function)에 대한 모델을 정해놓고 
    - 데이터들로부터 모델의 파라미터만 추정하는 방식이다. 

예를 들어, '일일 교통량'이 정규분포를 따른다고 가정해 버리면 관측된 데이터들로부터 **평균**과 **분산**만 구하면 되기 때문에 밀도추정 문제가 비교적 간단한 문제가 되어 버린다.


### 1.1 최대 우도(ML)
- 1992년 R.A. Fisher 제안

- 사전 확률 고려 안함

- 최대 우도(ML)는 일어날 가능성(우도)이 가장 큰 것을 나타냄
    - 즉, 관측된 랜덤 표본에 대한 여러 가설 중 관측결과에 따른 우도함수 값이 최대인 것

- 최대 우도 추정치(MLE: ML Estimator)
    - L($$\Theta$$)를 최대로하는 $$\Theta$$ 값에 대한 추정치를 $$\hat\Theta$$  이라하면,
    - 이때의 $$\hat\Theta$$를 최대우도 추정치 또는 최대 가능도 추정량 (ML Estimator, MLE) 라고함

- 확률 추정 문제 = 최대 우도를 갖는 매개 변수를 찾는 문제 = 최대 우도 방법(ML method)
    - 즉, 주어진 X를 발생 시켰을때 가능성이 가장 높은 매개변수 집합($$\Theta$$)를 찾아라 

![](http://i.imgur.com/vZTi68X.png)
위 예에서 
- P(X| Θ1)>P(X| Θ2)
- 최대 우도를 갖는 Θ는?

만약 추출된 표본으로부터 likelihood function을 구했다면,  
likelihood function을 최대로 만들어주는 모수를 찾는 것

| $$L(\hat\theta; x_1,...,x_n) = \max_{\theta \in \Xi}L(\theta; x_1,...,x_n)$$ |
| --- |
| - $$\Xi$$는 가능한 모든 모수의 집합, $$\theta$$는 모수 |

주어진 샘플  x 에 대해 우도를 가장 크게 해 주는 모수  θ 를 찾는 방법



최대 우도법 : 선형회귀분석에서는 최소제곱법을 통해 회귀식을 추정하였다. 그러면 종속변수와 독립변수가 선형 관계에 놓여 있지 않는 일반화 선형모형에서는 어떤 방식으로 회귀식을 추정할 수 있을까? 일반화 선형모형에서는 대개 최대우도법\(maximum likelihood method\)을 이용하여 회귀식을 추정한다







###### 공식 : 

$$

MLE = \hat\Theta= argmax_\Theta p(X|\Theta)

$$

###### 문제 풀이 

|원식|$$argmax_\Theta p(X|\Theta)$$|
|-|-|
|풀어쓰기|![](http://i.imgur.com/CURnf16.png)<br>- $$X={x_1,x_1,..,x_N}$$|
|로그 우도 변환($$\ln$$취하기)|![](http://i.imgur.com/yhBVK32.png)<br>- $$\prod{}{} \rightarrow \sum{}{}$$로 변한건 Log의 곱성질 때문인가?|
|최적화 문제 이므로 미분 이용|![](http://i.imgur.com/36fk8Wr.png)<br>- 최적문제 : L($$\Theta$$)의 도함수를 0으로 두고 풀어 구한 답이 $$\hat\Theta$$

##### 예제

정규 분포에서 분산\($$\sigma^2 = 1$$\), 샘플\($$x_1 =1 $$\)은 알고 있으나 평균\($$\mu$$\)을 모를 경우,

* 어떤 $$\mu(\mu =-1, \mu =0, \mu=1)$$ 값이 가장 가능성\(우도\)이 있어 보이는가?
* 이 세가지 $$\mu$$값에 대해$$x_0$$이 나올 확률이 바로 우도 이다. 

![](http://i.imgur.com/DC9Eadn.png)

1일 경우의 우도가 가장 크다. 따라서 MLE에 위한 추정값은 1이다.

> 구현시는 우도를 로그변환한 로그 우도\(Log likelihood\)함수로 변환 하여 계산 용이 하도록 함\(곱셈-&gt;덧셈\)





###### ML 활용 가능 알고리즘 
- EM 알고리즘 
- Baum-Welch 알고리즘

> ML은 메타 휴리스틱(=응용에 따라 내부 알고리즘 대체) 이므로 


###### \[참고\] MLEs를 이용하여 모수 population parameter 를 추정하는 방법

* [Geometric distribution의 모수 추정하기](http://m.blog.daum.net/gongdjn/123)
* [Poisson distribution의 모수 추정하기](http://m.blog.daum.net/gongdjn/124)
* [Uniform distribution의 모수 추정하기](http://m.blog.daum.net/gongdjn/125)
* [Normal distribution의 모수 추정하기](http://m.blog.daum.net/gongdjn/126)
* Gamma distribution의 모수 추정하기




### 1.2 MAP방법

- 사전 확률 고려함 

- ML은 P($$\Theta$$)가 균일하다고 가정하고 있다.(사전확률 고려 안함)

- 만약 P($$\Theta$$) 정보가 사용가능하고 균일 하지 않다면 P($$\Theta$$)를 식에 추가 하여야 한다. 

![](http://i.imgur.com/R6Nul8J.png)

식에서 P($$x_i \mid \Theta$$)를 우도로 , P($$\Theta$$)를 사전 확률로 본다면 P($$x_i \mid \Theta$$)P($$\Theta$$)는 사후확률로 간주 할수 있다. 
- 이 수식을 풀어서 최적의 매개 변수를 찾는 방법을 MAP라 한다. 

||ML방법|MAP방법|
|-|-|-|
|사전확률|균일|균일하지 않음|
|공식|![](http://i.imgur.com/yhBVK32.png)|![](http://i.imgur.com/R6Nul8J.png)|
|최적해 위치가 다름|![](http://i.imgur.com/T5qe9aZ.png)|![](http://i.imgur.com/qsTiGif.png)<br>- 사전 확률이 균일하지 않은 영향을 받아 최적해가 오른쪽으로 이동|





## 2. 비 모수적 방법 
- 창의 크기 고정 유무 

### 2.1 히스토 그램 

히스토그램을 확률 분포로 사용하기 위해서는 각 빈의 값을 N으로 나누어 정규화해 주면 된다. 

단점
- bin의 경계에서 불연속성이 나타난다는 점
- bin의 크기 및 시작 위치에 따라서 히스토그램이 달라진다는 점
- 고차원(high dimension) 데이터에는 메모리 문제 등으로 사용하기 힘들다는 점 등의 문제점


출처: http://darkpgmr.tistory.com/147 [다크 프로그래머]

### 2.2 커널 밀도 추정

- 커널함수(kernel function)를 이용하여 히스토그램 방법의 문제점을 개선한 방법
    
- 대표적 KDE방법 : 파젠 창

#### 가. 히스토그램을 확장하여 확률밀도를 구하는 방법 

###### Step 1. 임의의 점 x에서 확률 값 추정 

크기가 H인 창을 이용하여 확률 구하기 $$ p(x) = \frac{1}{h^d}\frac{k_x}{N}$$
- h : 창크기
- d : 차원
- k : 샘플의 갯수
- N : 전체 샘플의 갯수 

![](http://i.imgur.com/HjTbt2P.png)


###### Step 2. 커널 함수를 이용하여 근접도에 따른 가중치 주기 

x에 가까운 정도에 가중치 주기 (eg. 그림 3.6 a에서 왼쪽 점이 오른쪽 점보다 x에 가까움)   

![](http://i.imgur.com/WO1Co5Z.png?)

계산함수(a)의 커널 함수 : $$k_x = \sum^N_{i=1}k\left(\frac{x-x_i}{h}\right) \rightarrow p(x)=\frac{1}{h^d}\frac{1}{N}\sum^N_{i=1}k\left(\frac{x-x_i}{h}\right)$$

매끄러운(b)의 커널 함수 : 가우시언함수 $$k(x) = \frac{1}{n}\sum^n_{i=1}\frac{1}{(\sqrt[h]{2\pi})^d}exp\left(-\frac{1}{2}\left(\frac{x-x_i}{h}\right)^2\right)$$


###### [참고] 대표적인 커널 함수 
> 커널함수(kernel function): 원점을 중심으로 대칭이면서 적분값이 1인 non-negative 함수 (eg. 가우시언함수)

- Uniform : 계단형태
- Gaussian : 계산의 편의성이 큰 커널 함수(추천 h = $$\left(\frac{4\sigma^5}{3n}\right)^\frac{1}{5}\approx 1.06\sigma n^{-1/5}$$ )
- Epanechnikov  : (위키피디아 추천) 가장 최적의 커널함수 

![](http://i.imgur.com/vUWfKw2.png)


#### 나. 단점 

- 차원의 저주에 자유 롭지 못함, 고차원이 되었을 때의 문제에서는 여전히 자유롭지 못합니다.

- 1차원에서 $$N_1$$개의 샘플이 필요하다면 d차원에서 $$N_1^d$$ 정도가 있어야 믿을만한 확률 분포를 얻으수 있다. 

- 해결 : KNN
    
    https://medium.com/mathpresso/mathpresso-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%8A%A4%ED%84%B0%EB%94%94-14-%EB%B0%80%EB%8F%84-%EC%B6%94%EC%A0%95-density-estimation-38fd7ef729bb

http://carstart.tistory.com/


### 2.3 K-최근접 이웃 추정 (k-NN Rule)

KDE의 방식 : 고정된 창, 창의 중심 x를 어디에 두느냐에 따라 창 안의 샘플의 개수가 달라진다. 
K-NNR의 방식 : 가변적 창, x를 중심으로 창을 씌우고 k개의 샘플이 창 안에 들어 올때까지 창의 크기를 확장한다. 

### 가. 확률 밀도 구하기 

![](http://i.imgur.com/rGCqf9W.png)
$$
p(x) = \frac{1}{h_x^d}\frac{k}{N}
$$

> KDE와 비교 하여 h는 x에 따라 변하므로 $$h$$를 $$h_x$$로 표기 하였다. 

#### 나. 단점 

창크기 조정을 위한 계산량이 많다. 

해결책 : 보르노이 도형, 훈련집합에 따라 특징 공간을 미리 여러 구간으로 나누어 놓음 

### 다. K-NN분류기 

기본 아이디어는 K-NNR을 활용 : x를 $$\omega_q$$로 분류하라, 이때 $$q=argmax_i k_i$$

$$
K-NN 분류기 = p(\omega_i \mid X) = \frac{P(\omega_i)p(X \mid \omega_i)}{p(X)} = \frac{k_i}{k}
$$

베이즈 공식에 적용 
- $$p(X \mid \omega_i) = \frac{k_i}{k_X^dN_i}$$ : 조건부확률(likelihood)
- $$P(\omega_i)=\frac{N_i}{N}$$ : 사전확률
- $$p(X) = \frac{k}{h_x^dN} $$

###### Step 1. 훈련 샘플 중에 x에 가장 가까운 k개를 찾는다. 

거리 척도 : 유클리디언거리, 마할라노비스 거리 척도 사용 

###### Step 2. k개가 속한 부류를 조사하여 가장 빈도가 높은 류를 $$\omega_q$$라 한다. 



## 3. 혼합모델(두개 이상의 분포) 

두개 이상의 서로 다른 확률 분포의 혼방법 합으로 X를 모델링 하는 

### 3.1 가우시언 혼합
http://blog.naver.com/kmkim1222/10187825620
http://mang_goo83.blog.me/20093305203
http://blog.naver.com/intencelove/20104400922
###### Step 1. 문제 정의 하기 

![](http://i.imgur.com/SGdcXkp.png)

- 주어진 것
    - 데이터 
    - eg. 24개의 샘플, N=24(위쪽 16개, 아래쪽 8개)

- 최적화 해야 하는것 
    - [Step 3] 에서 정의

- 추정해야 하는 매개 변수 
    - 가우시언 개수 K : 자동 결정 or 임의 결정 
    - k번째 가우시언의 매개 변수 ($$평균벡터:\mu_k, 공분산행렬:\sum_k$$)
    - k번쨰 가우시언의 가중치 $$\pi_k$$

###### Step 2. 모델링 하기 
![](http://i.imgur.com/mK8DsQL.png) 

$$
p(x) = \frac{1}{3}N(\mu_1, \sum_1)+\frac{2}{3}N(\mu_2, \sum_2)
$$


- 가우시언 개수 K :  임의 결정 2개 
- k번째 가우시언의 매개 변수 ($$평균벡터:\mu_k, 공분산행렬:\sum_k$$)
- k번쨰 가우시언의 가중치 $$\pi_k$$: 1/3 , 2/3 

###### Step 3. 최적화 대상 정의 하기 

- [Step 2] 일반화 $$p(x) = \frac{1}{3}N(\mu_1, \sum_1)+\frac{2}{3}N(\mu_2, \sum_2) \rightarrow \sum^K_{k=1}\pi_kN(x \mid \mu_k, \sum_k) $$
    - $$N(x \mid \mu_k, \sum_k)$$: x의 함수임을 명확히 하기 위해 재 표기 
    - $$\pi_k$$: 혼합 계수 
    
- 최적화 = 주어진 데이터(X)를 가지고 매개변수($$\Theta = \pi, \mu, \sum$$) 최적화 

###### Step 4. MLE(최대 우도) 문제로 발전 

![](http://i.imgur.com/c1ipmU6.png)
> 표기법 문제로 Θ는 $$\theta$$와 같음 

최종적으로 `X에 대해 최대 우도를 갖는 Θ를 찾는 문제`

$$
\hat\theta = argmax_{\Theta} \ln p(X \mid \Theta)
$$

###### Step 5. 최적화 문제 풀이 

- 미분 이용 : 혼합모델은 $$\mu, \sum$$외에 $$\pi, k$$까지 풀어야 함으로 미분으로는 적합치 않음 

- EM 알고리즘(1977년) 
    - E 단계(Expectation) : 샘플이 어느 가우시언에 속하는 지 추정 
    - M 단계(Macimization) : 매개 변수 집합 $$\Theta$$을 추정 

### 3.1 가우시언 혼합 최적화 풀이 (EM 알고리즘)

![](http://i.imgur.com/61gYrcd.png)

#### A. E 단계

샘플 $$x_i$$가 어느 가우시언에 소속되었는지 나타내기 위한 새로운 개념(벡터 z) 도입 필요 

|벡터 z = $$(z_1, z_2,...z_k)^T$$|
|-|
|- eg. 샘플 $$x_i$$가 j번째 가우시언에서 발생 했다면 $$z_j = 1$$, 나머지는 모두 0,<br>- 즉, 벡터 z는 하나만 1을 가지고 나머지는 모두 0 (cf. 핫 인코딩)|

j 번째 가우시언에서 샘플 $$x_i$$가 발생할 확률 =  $$x_i$$의 조건부 확률 (우도) 
$$
p(x_i \mid z_j =1) = N(x_i \mid \mu_j, \sum_j) 
$$

> 벡터 z는 $$\Theta$$와 다르다. 
> - $$\Theta$$는 알고리듬에 직접 이용 되지만, z는 동작과정에서 활용되는 은닉변수(latent Variable) 

샘플 $$x_i$$가 관찰되었을때 j번째 가우시언에서 발생했을 조건부 확률(사후확률)
$$
p(z_j=1 \mid x_i)= \frac{P(x_j =1)p(x_i \mid z_j =1)}{p(x_i)}
$$

![](http://i.imgur.com/aCXQLJl.png?1)
> 추후 식 유도 과정 살펴 보기 

###### Step 1. 문제 재 확인 

$$\hat\Theta = argmax_\Theta \ln(p(X\mid \Theta)$$ 의 최적화 문제를 풀기 위해 아래식을 미분하여 얻은 도함수를 0으로 두 해를 구한다. 

![](http://i.imgur.com/20ovNxF.png)


###### Step 2. 평균 벡터$$\mu_j$$ 구하기 



