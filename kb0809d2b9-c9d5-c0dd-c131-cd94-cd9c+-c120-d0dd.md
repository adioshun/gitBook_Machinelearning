# 특징 생성 

특징 생성 = 특징 추출 + 특징 선택 

특징생성 정의: 외부 세하는 패턴으로부터 특징 벡터를 만드는 전체 과정 계에 존재

특징은 두 가지 기준에서 우수 해야 한다. 
- 분별력(Discriminatory): 서로 다른 부류를 잘 분별해 주어야 한다. 
- 차원(Demensionality): 차원이 낮을수록 계산 효율이 좋다. 

![](http://i.imgur.com/its92PD.png)

## 1. 특징 추출 
- 정의 : 실제 세계를 수학 세계로 바꾸어 주는 추상화 작업 

- 특징 : 추출 알고리즘들이 각 응용 분야에 의존 적이다. 

### 1.1 응용 분야별 특징 추출 방법 

#### A. 영역(Region)에서의 특징 추출 

응용분야 : 이미지 인식, 문자인식, 의료 영상 인식 

![](http://i.imgur.com/NhNSmfE.png)

##### 가. 모양에 관련한 특징 

1. 면적 : 모멘트 이용하여 계산 $$\alpha = m_{00}$$

2. 중점 : 모멘트 이용하여 계산 $$(\bar x, \bar y) = \left( \frac{m_{10}}{\alpha},\frac{m_{01}}{\alpha} \right)$$

3. 행분산 : 중심 모멘트 이용하여 계산 $$v_{rr} = \frac{\bar m_{20}}{\alpha}$$

4. 열분산 : 중심 모멘트 이용하여 계산 $$v_{cc} = \frac{\bar m_{02}}{\alpha}$$

5. 혼삽 분산 : 중심 모멘트 이용하여 계산 $$v_{rc} = \frac{\bar m_{11}}{\alpha}$$

6. 영역 둘례 : 체인코드를 이용하여 계산 $$p=n_{even}+ n_{odd}\sqrt{2} $$

7. 둥근 정도 : 영역의 둘례 + 면적 이용하여 계산  $$r=\frac{4\pi\alpha}{p^2}$$

||모멘트|중심모멘트|
|-|-|-|
|식|$$m_pq = \sum_{(x,y) \in R}x^py^q$$|모멘트의 중점을 원점으로 간주 하고 계산<br> $$\bar{m}_{pq} = \sum_{(x,y) \in R (x-\bar x)^p(y-\bar y)^q} $$
|활용|먼적(Area)구하기, 중점(Centroid)구하기|행/열/혼합 분산 정도|

![](http://i.imgur.com/UUXhlhV.png)

##### 나. 투영에 관련한 특징 

영역을 가로 방향과 세로 방향으로 투영하여 특징을 추출할 수 있다. 



###### 1. 특징벡터 
![](http://i.imgur.com/njtTJmP.png)

![](http://i.imgur.com/PjS2r1b.png)

특징 벡터 $$X = (3,3,3,3,4,4,1,2,3,6,5,3)^T$$, N+M개의 특징을 얻게 된다. (eg. 6+6 = 12차원)


###### 2. 프로파일 특징 
2(N+M)개의 특징을 얻게 된다. 

![](http://i.imgur.com/qjxbX4t.png)

![](http://i.imgur.com/kSTtnFI.png)

![](http://i.imgur.com/4LWktxy.png)


#### B. 파형(waveform)에서의 특징 추출 

응용분야 : 지진파, 주식곡선, 성적 곡선

![](http://i.imgur.com/WB6f7WD.png)

위 그림에서 a(신호 s1)과 b(신호 s2)는 서로 다른 신호 이다. 

신호는 기저 함수(c,d)의 선형 결합으로 표현 할수 있다. 

- $$S_1(x) = 0.5g_1(x)+1.5g_2(x)$$
- $$S_2(x) = 1.5g_1(x)+0.5g_2(x)$$

기저 함수의 계수(0.5 OR 1.5)로 신호를 구분 할수 있음 = 특징으로 사용 가능 
    - 하지만, 실제로는 기저 함수의 `계수`를 미리 알수 없다. 
    - `퓨리에 변환`을 이용하면 계산 할 수 있다. 

##### 가. 이산 퓨리에 변환 
목적 : 입력 신호 s(i)를 f(u)로 변환 = 시간공간을 주파수 공간으로 변환 
    - i = 시간, u = 주파수 
    - f(u) : u번째 기저 함수의 계수 

![](http://i.imgur.com/t5lpDvj.png)

##### 나. 예제 (1차원)

###### Step 1. 아날로그 신호를 샘플링 하여 디지털 신호로 변환 

![](http://i.imgur.com/UJO5cDC.png)
$$s=(1.7,0.6,1.5,1.2)^T$$

###### Step 2. 퓨리에 공식에 적용 

![](http://i.imgur.com/9wQqnfh.png)
![](http://i.imgur.com/YtICE1W.png)
- 실수부 : $$real(f(u))$$
- 허수부 : $$imag(f(u))$$

###### Step 3. 퓨리에 특징p(u) 구하기 

![](http://i.imgur.com/dATzzjV.png)

> 파워스트럼 값 = 퓨리에 특징 

![](http://i.imgur.com/FPYrQnF.png)

##### 나. 예제 (2차원)

1차원과 과정은 같으며 퓨리에 변환과 파워 스펙트럼 계산 부분만 아래 식으로 변환 한다. 

![](http://i.imgur.com/DTzNabS.png)

##### 다. 퓨리에 기술자 

- n*n 영상에 퓨리에 변환 적용하여 특징 추출

- 영역 경계 상의 점을 복소수로 표현한 뒤에 2 차원 퓨리에 변환 적용 


> 자세한 내용은 책 참고 패턴인식, 오일석, 274P


### C. 시계열 신호(Time Series)에서의 특징 추출 

응용분야 : 음성파, 지진파, 주식 시세 

특징 : 동적인 성질을 가지고 있음 (cf. 영상신호 = 정적인 성질)

절차 : 시계열 신호를 프레임 단위로 나누고 각각의 프레임에서 특징 벡터를 추출 

추출 알고리즘 : 퓨리에 변환 활용 (시계열 신호 역시 주파수 + 진폭 이므로)
    - 최근에는 퓨리보다 분별력이 우수한 `갭스트럼(Cepstrum)` 특징 주로 이용 


### 1.2 PCA를 이용한 특징 추출 방법 
 
- 기존[1.1 응용 분야별 특징 추출 방법]은 주어진 데이터에서 바로 특징 추출 

- PCA는 훈련 집합을 이용하여 매개 변수를 추정하고 이를 변환식에 적용 

> PCA 특징 추출은 Karhumen-loeve(KL)변환, Hotelling 변환이라고도 불리운다. 

#### A. 개요 

목적 : PCA는 데이터(D차원)를 낮은 차원의 특징 벡터 X로(d차원) 변환 (d<D)
    - 변환은 단위벡터 u축으로 `투영`함으로써 가능 
    - 이때 정보 손실을 최소화 하여야 함 
    
활용 분야 
- 특징 추출 (eg. 얼굴 인식)
- 차원 축소 (cf. Fisher의 LDA)
- 데이터 압축 (eg. d차원을 작게 할수록 압축률 커지고, 데이터 손실 커짐)
- 데이터 시각화 (eg. 4차원 이상의 데이터를 d=2로 변경하여 이해 하기 쉬워짐)    

PCA알고리즘
-     
            
공식 : $$\hat x = u^Ts$$
- s : 데이터 
- u : 투영 되는 축 
- $$\hat x$$ : 투영된 값 

최적화 요소 : `분산`, 원래 공간에 퍼져 있는 정도를 투영된 공간에서도 얼마나 잘 유지 하는가 

#### B. 예제

![](http://i.imgur.com/Ivu5S1I.png)

데이터 $$X = {(2,1)^T,(2,4)^T,(4,1)^T,(4,3)^T} \Rightarrow$$ 속 빈 점 

투영 되는 축 
- a : $$u^T=(1,0)$$ 축
- b : $$u^T=(0,1)$$ 축
- c : $$u^T=(a/\sqrt{2},2/\sqrt{2})$$ 축

투영된 값 
- a 축으로 투영 : 분산 1.0
    - 투영시 $$S_2,S_1$$은 2로변환, $$S_3, S_4$$는 4로 변환 된다. 
    - 즉, $$S_2,S_1$$가 같은 점으로 거리/위치 등의 `정보 손실`이 발생 한다. 

    ![](http://i.imgur.com/NPH0WvR.png)

- c 축으로 투영 : 분산 1.0938

    ![](http://i.imgur.com/rFheRiH.png)


정보 손실
- a축으로 투영시 $$S_2,S_1$$은 2로변환, $$S_3, S_4$$는 4로 같은 한점으로 표시 되어 거리/위치 등의 `정보 손실`이 발생

- a(분산 1.0)보다  c(분산 1.0938)가 정보 손실이 더 적다 



#### C. 최적화된 투영 축 찾기 (1개축)

###### Step 1. 최적화 목표 정의 

변환된 샘플들의 분산을 최대화 하는 축 

###### Step 2. 분산을 계산 하는 법 찾기

![](http://i.imgur.com/u9Hc4BN.png)

평균은 원래 신호들의 평균벡터$$\bar S$$를 구한다음 그것을 $$u$$로 투영 한것이고 
분산은 이 값을 활용하면 구할수 있음

###### Step 3. 최적화 함수 정의 

목표 재 정의 : 식(30)의 분산 $$\hat \sigma^2$$을 최대화 하는 u를 찾아라 
    - 이때, u가 단위 벡터 이므로 $$uu^T=1$$이라는 조건을 만들수 있음 (???억지로라도 조건을 만들어야 하나?)
    
조건부 최적화 문제(=라그랑제 승수활용)
![](http://i.imgur.com/0DpuOk4.png)


###### Step 4. 미분하고 수식 정리 

![](http://i.imgur.com/999tDvp.png)
> 3번째의 가로안은 공분산 행렬이다. 이를 4번째 줄에서 $$\sum(공분산 기호)$$로 대치 하였다. 

###### Step 5. 최종식 

라그랑제 상수 L(u)를 최대화 하는 U는 $$\frac{\partial L(U)}{\partial} =0$$을 만족 해야 하므로 , $$\frac{\partial \sum\mu - 2\lambda \mu}{\partial} =0  $$

즉, $$\sum\mu = 2\lambda \mu $$이다. 이를 잘 살펴 보면 
- $$\mu$$는 공분산 행렬 $$\sum$$의 고유 벡터(eigenvector)
- $$\lambda$$는 고유 값(eigenvalue)

|결론: 훈련집합의 공분산 행렬 구하기 -> 공분산 행렬의 고유 벡터 구하기 -> 최대 분산 u |
|-|


###### [예제 문제]

![](http://i.imgur.com/LMGu0pY.png)
투영 변환된 점들의 평균은 1.2122이고, 분산은 1.7688이다. 
첫 C축으로 투영된 분산 1.0938보다 나은 수치임을 확인 가능 

#### D. 최적화된 투영 축 찾기 (d개 축)

###### Step 1. 사전 정보 정의 
- 실제 문제 풀이 에서는 D차원을 d차원으로 줄여야 하므로 d개의 축이 필요 하다. 

- 공분산 행렬은 D x D크기를 가지므로 이 행렬의 고유 벡터는 D개가 존재 한다. 

- 고유 벡터 성질에 따라 이들은 모두 서로 수직(Orthogonal)이다.$$즉, \mu_i^T\mu_j = 0, i \neq j$$이다. 

- 고유 값이 클수록 중요 값이 크므로 정렬 후 상위 d개를 고유 벡터 선별 


###### Step 2. 변환행렬 U 구성

- 선별된 고유 벡터를 $$\mu_1, \mu_2, \mu_3,..,\mu_d$$로 표기 하고 `주성분` 이라고 부른다

- d개의 주성분을 이용하여 변환행렬 U를 구성한다. 

     ![](http://i.imgur.com/sj6E0Jf.png?1)

> U의 한 행을 차지 하는 $$\mu_i^T$$는 크기가 D인 행 벡터 이다. 따라서 U는 d*D행렬이다. 

###### Step 3. 최종식 도출 

$$
x = Us
$$

> 역변환행력을 이용하여 `복원`도 가능 


### 1.3 [참고] LDA (PCA확장)

> LDA는 특징 추출 보다는 Classifier성격이 강하나 PCA와 비슷하기에 같이 다룸 

#### A. 개요 

분류기 알고리즘, 원리는 PCA와 비슷 

PCA와 Fisher LD는 목표가 다름
- PCA는 정보 손실 최소화 (샘플의 부류 정보 사용 안함)
- Fisher LD는 분별력을 최대화 (샘플의 부류 정보 사용함)


공식 : $$ y=W^TX (\because 분류기 설계이므로 기호 다름)$$ 
- X : 데이터 
- u : 투영 되는 축 
- $$\hat x$$ : 투영된 값 

최적화 요소 : $$\frac{부류간 퍼짐}{부류내 퍼짐}$$ = 같은 부류의 샘플(부류내)은 덜 퍼지고, 다른 부류의 샘플(부류간)은 더 퍼질수록 높은 점수 


- 부류간 퍼짐(Between-class scatter): $$\mid \bar m_1 - \bar m_2 \mid = \mid W^Tm_1 - W^Tm_2 \mid = \mid W^T(m_1 - m_2) \mid$$ 
    - $$m_i(원래 공간의 평균점) : \frac{1}{N}\sum_{x \in \omega_i}X$$
    - $$\bar m_i(투영후의 평균점) : \frac{1}{N}\sum_{y \in \omega_i}y = \frac{1}{N}\sum_{y \in \omega_i}W^TX = W^Tm_i$$


- 부류내 퍼짐(Within-class scatter): $$\bar s_1^2 +\bar s_2^2 $$
    - 투영된 점들이 평균점에 얼마나 가까지 분포 하는가
    - $$\bar s_i^2 = \sum_{y \in \omega_i}(y-\bar m)^2$$

#### B. 예제 

![](http://i.imgur.com/EczOAvL.png)


데이터 $$X = {(2,1)^T,(2,4)^T,(4,1)^T,(4,3)^T} \Rightarrow$$ 속 빈 점 
    - 분류 : 파란색 $$\omega_1$$, 검은색 $$\omega_2$$

투영 되는 축 
- a : $$W^T=(1,0)$$ 축
- b : $$W^T=(0,1)$$ 축
- c : $$W^T=(a/\sqrt{2},2/\sqrt{2})$$ 축

분별력(cf. PCA는 정보 손실에 초점)
- a축으로 투영시 $$X_1,X_2$$은 한점으로 변환, $$X_3, X_4$$는 같은 한점으로 표시 되어 분별력 사라짐 

- b축으로 투영시 분별 가능 (두 분류 사이 간격 큼)

- C축으로 투영시 분별 가능 (두 분류 사이 간격 작음)

- [결론] C측이 분별력이 큼 



#### C. 최적화된 투영 축 찾기 

###### Step 1. 최적화 목적 함수 정리 

$$
J(W) = \frac{부류간 퍼짐}{부류내 퍼짐} = \frac{\mid \bar m_1 - \bar m_2 \mid }{\bar s_1^2 +\bar s_2^2}
$$

|![](http://i.imgur.com/dyjp3Gf.png)|![](http://i.imgur.com/tjgZIlH.png)|
|-|-|
|행렬$$S_B$$를 부류간 퍼짐 행렬이라 부름<br>(Between-class scatter matrix)|행렬$$S_W$$를 부류내 퍼짐 행렬이라 부름<br>(Within-class scatter matrix)

$$
J(W) = \frac{W^TS_BW}{W^TS_WW}
$$

###### Step 2. 목적함수 최적화  

$$
\frac{\partial J(W)}{\partial W} = 0 \Rightarrow (W^TS_WW)S_BW = (W^TS_BW)S_WW

$$

###### Step 3. 식 정리 및 최종 식 

![](http://i.imgur.com/xLSMvzL.png)
> 상세 풀이 식 : 패턴인식, 오일석, 291P

Fisher의 선별 분별(LD:Linear Discriminant): $$W= \alpha S_W^{-1}(m_1 - m_2)$$

###### [예제 문제]

![](http://i.imgur.com/rW6Tody.png)


### 1.4  t-SNE

t-분포 확률적 임베딩(t-SNE)은 데이터의 차원 축소에 사용되는 기계 학습 알고리즘 중 하나로, 제프리 힌튼에 의해 개발되었다

> [In Raw Numpy: t-SNE](https://nlml.github.io/in-raw-numpy/in-raw-numpy-t-sne/)

## 2. 특징 선택(Feature Selection

특징 선택 목적 : 분별력을 유지/향상 시키면서 특징 벡터의 차원을 줄이기 

특징 선택은 d개의 특징을 갖는 특징 집합 F에서 $$\hat d$$개를 갖는 최적의 특징 부분 집합(Feature subset) $$\hat F$$ 를 선택 하는 문제 

### 2.1 특징의 분별력 

분별력(Discriminatory power) : 특징 벡터 X가 얼마나 좋은지의 척도, 부류 분리 능력


![](http://i.imgur.com/5kIWdbA.png)
**[ 특징 벡터는 부류내 분산은 작을수록, 부류간 분산은 클수록 좋다. a>b>c ]**


### 2.2 분별력 측정 기준 

#### A. 다이버젼스(Divergence)

> 다이버전스:서로 다르게 움직이는 현상

두 부류의 확률 분포 $$p(X \mid \omega_i)$$와 $$p(X \mid \omega_j)$$의 거리가 멀수록 X는 두 부류를 잘 분별해 주므로 좋은 특징 벡터 이다. 

![](http://i.imgur.com/ofp3RdH.png)
**[b가 더 좋은 특징 벡터 이다.]**

두 확률 분포간의 거리 측정 방법 : KL(Kullback-Leibler) 다이버전스 

- 두개 부류 : $$d_{ij}=KL(p(X \mid\omega_i), p(X\mid\omega_j))+KL(p(X\mid\omega_j),p(X\mid\omega_i))$$

- M개 부류 : $$d=\sum_{i=1}^{M}\sum_{j=1}^{M}P(\omega_i)P(\omega_j)d_{ij}$$



단점/제약

- 확률 분포를 알고 있을 때만 적용 가능 

- 확률 분포 추정을 할수는 있지만 그럴경우 `차원의 저주`의 문제를 가짐 


#### B. 훈련 샘플의 거리 

샘플을 가지고 직접 측정 가능 (cf. KL 다이버전스비해 현실적)

부류 $$\omega_i$$에 속한 샘플 집합을 $$X_i$$라 하고, $$X-i$$의 k번째 샘플을 $$X_i^k$$로 표기

- 두개 부류 : $$d_{ij}=\frac{1}{N_iN_j}\sum_{k=1}^{N_i}\sum_{m=1}^{N_j}dist(X_i^k,X_j^m)$$

    - $$N_i$$: $$X_i$$의 샘플 개수 
    
    - $$dist(X_i^k,X_j^m)$$: 두점 간의 거리, `마할라노비스거리`또는 `유클리드거리`척도 사용 

- M개 부류 : $$d= \sum_{i=1}^M\sum_{j=1}^M P(\omega_i)P(\omega_j)d_{ij}$$


### 2.3 특징 선택을 위한 탐색 알고리즘 

Feature selection은 $$2^d$$개의 해로 정의되는 방대한 탐색 공간을 가진다. 이 탐색 공간을 효율적으로 뒤질 것인가가 중요한 문제 이다. 

#### A. 전역 탐색(Global Search) 알고리즘 

개요 : 전체 특징 공간을 탐색 

특징
- 최적해를 보장 
- 비 효율적(시간안에 끝나지 않을 가능성 있음)

종류 
- 낱낱 탐색(Exhaustive Search) 알고리즘 : 기본 전역 탐색 알고리즘, 가능성 여부 없이 모든 것 탐색 
- 한정 분기(Btansh 알고리즘 : 가능성이 없다고 판단되는것은 배제 하고 탐색

#### B. 순차 탐색(Sequential Search) 알고리즘 

개요 
- 전체 특징 집합 F에서 임의의 특징 집한 $$f_A$$를 선택하고, 

- 나머지 특징 중에서 하나씩 add / rem 수행 

- 성능 변화 관찰

종류 
- SFS(Sequential forward search) 알고리즘: $$f_A$$에서 하나씩 추가 하며 찾기 
- SBS(Sequential backward search) 알고리즘: F에서 하나씩 빼며 찾기 
- PTA(plus-p-take-away-q) 알고리즘: NULL에서 추가와 제거를 번갈아 가면서 찾기 (고정된 크기의 특징들)
- SFFS(Sequential forward floating search) 알고리즘 : $$f_A$$에서 추가와 제거를 번갈아 가면서 찾기 (동적인 크기의 특징들)
- SBFS(Sequential backward floating search) 알고리즘 : F에서 추가와 제거를 번갈아 가면서 찾기 (동적인 크기의 특징들)

성능 비교 
- 전방 탐색 : SFS < PTA(p>q) < SFFS 
- 후방 탐색 : SBS < PTS(p<q) < SBFS

#### C. 통계적 탐색 알고리즘 

- 시뮬레이티드 어닐링

- 유전 알고리즘 


















