# KB0809\_특징생성\_추출+선택

특징 생성 = 특징 추출 + 특징 선택

특징생성 정의: 외부 세하는 패턴으로부터 특징 벡터를 만드는 전체 과정 계에 존재

특징은 두 가지 기준에서 우수 해야 한다.

* 분별력\(Discriminatory\): 서로 다른 부류를 잘 분별해 주어야 한다. 
* 차원\(Demensionality\): 차원이 낮을수록 계산 효율이 좋다. 

![](http://i.imgur.com/its92PD.png)

## 1. 특징 추출

* 정의 : 실제 세계를 수학 세계로 바꾸어 주는 추상화 작업
* 특징 : 추출 알고리즘들이 각 응용 분야에 의존 적이다.

### 1.1 응용 분야별 특징 추출 방법

#### A. 영역\(Region\)에서의 특징 추출

응용분야 : 이미지 인식, 문자인식, 의료 영상 인식

![](http://i.imgur.com/NhNSmfE.png)

**가. 모양에 관련한 특징**

1. 면적 : 모멘트 이용하여 계산 $$\alpha = m_{00}$$
2. 중점 : 모멘트 이용하여 계산 $$(\bar x, \bar y) = \left( \frac{m_{10}}{\alpha},\frac{m_{01}}{\alpha} \right)$$
3. 행분산 : 중심 모멘트 이용하여 계산 $$v_{rr} = \frac{\bar m_{20}}{\alpha}$$
4. 열분산 : 중심 모멘트 이용하여 계산 $$v_{cc} = \frac{\bar m_{02}}{\alpha}$$
5. 혼삽 분산 : 중심 모멘트 이용하여 계산 $$v_{rc} = \frac{\bar m_{11}}{\alpha}$$
6. 영역 둘례 : 체인코드를 이용하여 계산 $$p=n_{even}+ n_{odd}\sqrt{2}$$
7. 둥근 정도 : 영역의 둘례 + 면적 이용하여 계산 $$r=\frac{4\pi\alpha}{p^2}$$

|  | 모멘트 | 중심모멘트 |
| :--- | :--- | :--- |
| 식 | $$m_pq = \sum_{(x,y) \in R}x^py^q$$ | 모멘트의 중점을 원점으로 간주 하고 계산  $$\bar{m}_{pq} = \sum_{(x,y) \in R (x-\bar x)^p(y-\bar y)^q}$$ |
| 활용 | 먼적\(Area\)구하기, 중점\(Centroid\)구하기 | 행/열/혼합 분산 정도 |

![](http://i.imgur.com/UUXhlhV.png)

**나. 투영에 관련한 특징**

영역을 가로 방향과 세로 방향으로 투영하여 특징을 추출할 수 있다.

**1. 특징벡터**

![](http://i.imgur.com/njtTJmP.png)

![](http://i.imgur.com/PjS2r1b.png)

특징 벡터 $$X = (3,3,3,3,4,4,1,2,3,6,5,3)^T$$, N+M개의 특징을 얻게 된다. \(eg. 6+6 = 12차원\)

**2. 프로파일 특징**

2\(N+M\)개의 특징을 얻게 된다.

![](http://i.imgur.com/qjxbX4t.png)

![](http://i.imgur.com/kSTtnFI.png)

![](http://i.imgur.com/4LWktxy.png)

#### B. 파형\(waveform\)에서의 특징 추출

응용분야 : 지진파, 주식곡선, 성적 곡선

![](http://i.imgur.com/WB6f7WD.png)

위 그림에서 a\(신호 s1\)과 b\(신호 s2\)는 서로 다른 신호 이다.

신호는 기저 함수\(c,d\)의 선형 결합으로 표현 할수 있다.

* $$S_1(x) = 0.5g_1(x)+1.5g_2(x)$$
* $$S_2(x) = 1.5g_1(x)+0.5g_2(x)$$

기저 함수의 계수\(0.5 OR 1.5\)로 신호를 구분 할수 있음 = 특징으로 사용 가능

* 하지만, 실제로는 기저 함수의 `계수`를 미리 알수 없다. 
* `퓨리에 변환`을 이용하면 계산 할 수 있다. 

**가. 이산 퓨리에 변환**

목적 : 입력 신호 s\(i\)를 f\(u\)로 변환 = 시간공간을 주파수 공간으로 변환

* i = 시간, u = 주파수 
* f\(u\) : u번째 기저 함수의 계수 

![](http://i.imgur.com/t5lpDvj.png)

**나. 예제 \(1차원\)**

**Step 1. 아날로그 신호를 샘플링 하여 디지털 신호로 변환**

![](http://i.imgur.com/UJO5cDC.png) $$s=(1.7,0.6,1.5,1.2)^T$$

**Step 2. 퓨리에 공식에 적용**

![](http://i.imgur.com/9wQqnfh.png) ![](http://i.imgur.com/YtICE1W.png)

* 실수부 : $$real(f(u))$$
* 허수부 : $$imag(f(u))$$

**Step 3. 퓨리에 특징p\(u\) 구하기**

![](http://i.imgur.com/dATzzjV.png)

> 파워스트럼 값 = 퓨리에 특징

![](http://i.imgur.com/FPYrQnF.png)

**나. 예제 \(2차원\)**

1차원과 과정은 같으며 퓨리에 변환과 파워 스펙트럼 계산 부분만 아래 식으로 변환 한다.

![](http://i.imgur.com/DTzNabS.png)

**다. 퓨리에 기술자**

* n\*n 영상에 퓨리에 변환 적용하여 특징 추출
* 영역 경계 상의 점을 복소수로 표현한 뒤에 2 차원 퓨리에 변환 적용

> 자세한 내용은 책 참고 패턴인식, 오일석, 274P

### C. 시계열 신호\(Time Series\)에서의 특징 추출

응용분야 : 음성파, 지진파, 주식 시세

특징 : 동적인 성질을 가지고 있음 \(cf. 영상신호 = 정적인 성질\)

절차 : 시계열 신호를 프레임 단위로 나누고 각각의 프레임에서 특징 벡터를 추출

추출 알고리즘 : 퓨리에 변환 활용 \(시계열 신호 역시 주파수 + 진폭 이므로\)

* 최근에는 퓨리보다 분별력이 우수한 `갭스트럼(Cepstrum)` 특징 주로 이용 

### 1.2 PCA를 이용한 특징 추출 방법

* 기존\[1.1 응용 분야별 특징 추출 방법\]은 주어진 데이터에서 바로 특징 추출
* PCA는 훈련 집합을 이용하여 매개 변수를 추정하고 이를 변환식에 적용

> PCA 특징 추출은 Karhumen-loeve\(KL\)변환, Hotelling 변환이라고도 불리운다.

#### A. 개요

목적 : PCA는 데이터\(D차원\)를 낮은 차원의 특징 벡터 X로\(d차원\) 변환 \(d&lt;D\)

* 변환은 단위벡터 u축으로 `투영`함으로써 가능 
* 이때 정보 손실을 최소화 하여야 함 

활용 분야

* 특징 추출 \(eg. 얼굴 인식\)
* 차원 축소 \(cf. Fisher의 LDA\)
* 데이터 압축 \(eg. d차원을 작게 할수록 압축률 커지고, 데이터 손실 커짐\)
* 데이터 시각화 \(eg. 4차원 이상의 데이터를 d=2로 변경하여 이해 하기 쉬워짐\)    

PCA알고리즘

* 
공식 : $$\hat x = u^Ts$$

* s : 데이터 
* u : 투영 되는 축 
* $$\hat x$$ : 투영된 값 

최적화 요소 : `분산`, 원래 공간에 퍼져 있는 정도를 투영된 공간에서도 얼마나 잘 유지 하는가

#### B. 예제

![](http://i.imgur.com/Ivu5S1I.png)

데이터 $$X = {(2,1)^T,(2,4)^T,(4,1)^T,(4,3)^T} \Rightarrow$$ 속 빈 점

투영 되는 축

* a : $$u^T=(1,0)$$ 축
* b : $$u^T=(0,1)$$ 축
* c : $$u^T=(a/\sqrt{2},2/\sqrt{2})$$ 축

투영된 값

* a 축으로 투영 : 분산 1.0
  * 투영시 $$S_2,S_1$$은 2로변환, $$S_3, S_4$$는 4로 변환 된다. 
  * 즉, $$S_2,S_1$$가 같은 점으로 거리/위치 등의 `정보 손실`이 발생 한다.

    ![](http://i.imgur.com/NPH0WvR.png)
* c 축으로 투영 : 분산 1.0938

  ![](http://i.imgur.com/rFheRiH.png)

정보 손실

* a축으로 투영시 $$S_2,S_1$$은 2로변환, $$S_3, S_4$$는 4로 같은 한점으로 표시 되어 거리/위치 등의 `정보 손실`이 발생
* a\(분산 1.0\)보다 c\(분산 1.0938\)가 정보 손실이 더 적다

#### C. 최적화된 투영 축 찾기 \(1개축\)

**Step 1. 최적화 목표 정의**

변환된 샘플들의 분산을 최대화 하는 축

**Step 2. 분산을 계산 하는 법 찾기**

![](http://i.imgur.com/u9Hc4BN.png)

평균은 원래 신호들의 평균벡터$$\bar S$$를 구한다음 그것을 $$u$$로 투영 한것이고 분산은 이 값을 활용하면 구할수 있음

**Step 3. 최적화 함수 정의**

목표 재 정의 : 식\(30\)의 분산 $$\hat \sigma^2$$을 최대화 하는 u를 찾아라

* 이때, u가 단위 벡터 이므로 $$uu^T=1$$이라는 조건을 만들수 있음 \(???억지로라도 조건을 만들어야 하나?\)

조건부 최적화 문제\(=라그랑제 승수활용\) ![](http://i.imgur.com/0DpuOk4.png)

**Step 4. 미분하고 수식 정리**

![](http://i.imgur.com/999tDvp.png)

> 3번째의 가로안은 공분산 행렬이다. 이를 4번째 줄에서 $$\sum(공분산 기호)$$로 대치 하였다.

**Step 5. 최종식**

라그랑제 상수 L\(u\)를 최대화 하는 U는 $$\frac{\partial L(U)}{\partial} =0$$을 만족 해야 하므로 , $$\frac{\partial \sum\mu - 2\lambda \mu}{\partial} =0$$

즉, $$\sum\mu = 2\lambda \mu$$이다. 이를 잘 살펴 보면

* $$\mu$$는 공분산 행렬 $$\sum$$의 고유 벡터\(eigenvector\)
* $$\lambda$$는 고유 값\(eigenvalue\)

| 결론: 훈련집합의 공분산 행렬 구하기 -&gt; 공분산 행렬의 고유 벡터 구하기 -&gt; 최대 분산 u |
| :--- |


**\[예제 문제\]**

![](http://i.imgur.com/LMGu0pY.png) 투영 변환된 점들의 평균은 1.2122이고, 분산은 1.7688이다. 첫 C축으로 투영된 분산 1.0938보다 나은 수치임을 확인 가능

#### D. 최적화된 투영 축 찾기 \(d개 축\)

**Step 1. 사전 정보 정의**

* 실제 문제 풀이 에서는 D차원을 d차원으로 줄여야 하므로 d개의 축이 필요 하다.
* 공분산 행렬은 D x D크기를 가지므로 이 행렬의 고유 벡터는 D개가 존재 한다.
* 고유 벡터 성질에 따라 이들은 모두 서로 수직\(Orthogonal\)이다.$$즉, \mu_i^T\mu_j = 0, i \neq j$$이다.
* 고유 값이 클수록 중요 값이 크므로 정렬 후 상위 d개를 고유 벡터 선별

**Step 2. 변환행렬 U 구성**

* 선별된 고유 벡터를 $$\mu_1, \mu_2, \mu_3,..,\mu_d$$로 표기 하고 `주성분` 이라고 부른다
* d개의 주성분을 이용하여 변환행렬 U를 구성한다.

  ![](http://i.imgur.com/sj6E0Jf.png?1)

> U의 한 행을 차지 하는 $$\mu_i^T$$는 크기가 D인 행 벡터 이다. 따라서 U는 d\*D행렬이다.

**Step 3. 최종식 도출**

$$
x = Us
$$

> 역변환행력을 이용하여 `복원`도 가능

### 1.3 \[참고\] LDA \(PCA확장\)

> LDA는 특징 추출 보다는 Classifier성격이 강하나 PCA와 비슷하기에 같이 다룸

#### A. 개요

분류기 알고리즘, 원리는 PCA와 비슷

PCA와 Fisher LD는 목표가 다름

* PCA는 정보 손실 최소화 \(샘플의 부류 정보 사용 안함\)
* Fisher LD는 분별력을 최대화 \(샘플의 부류 정보 사용함\)

공식 : $$y=W^TX (\because 분류기 설계이므로 기호 다름)$$

* X : 데이터 
* u : 투영 되는 축 
* $$\hat x$$ : 투영된 값 

최적화 요소 : $$\frac{부류간 퍼짐}{부류내 퍼짐}$$ = 같은 부류의 샘플\(부류내\)은 덜 퍼지고, 다른 부류의 샘플\(부류간\)은 더 퍼질수록 높은 점수

* 부류간 퍼짐\(Between-class scatter\): $$\mid \bar m_1 - \bar m_2 \mid = \mid W^Tm_1 - W^Tm_2 \mid = \mid W^T(m_1 - m_2) \mid$$ 
  * $$m_i(원래 공간의 평균점) : \frac{1}{N}\sum_{x \in \omega_i}X$$
  * $$\bar m_i(투영후의 평균점) : \frac{1}{N}\sum_{y \in \omega_i}y = \frac{1}{N}\sum_{y \in \omega_i}W^TX = W^Tm_i$$
* 부류내 퍼짐\(Within-class scatter\): $$\bar s_1^2 +\bar s_2^2$$
  * 투영된 점들이 평균점에 얼마나 가까지 분포 하는가
  * $$\bar s_i^2 = \sum_{y \in \omega_i}(y-\bar m)^2$$

#### B. 예제

![](http://i.imgur.com/EczOAvL.png)

데이터 $$X = {(2,1)^T,(2,4)^T,(4,1)^T,(4,3)^T} \Rightarrow$$ 속 빈 점

* 분류 : 파란색 $$\omega_1$$, 검은색 $$\omega_2$$

투영 되는 축

* a : $$W^T=(1,0)$$ 축
* b : $$W^T=(0,1)$$ 축
* c : $$W^T=(a/\sqrt{2},2/\sqrt{2})$$ 축

분별력\(cf. PCA는 정보 손실에 초점\)

* a축으로 투영시 $$X_1,X_2$$은 한점으로 변환, $$X_3, X_4$$는 같은 한점으로 표시 되어 분별력 사라짐
* b축으로 투영시 분별 가능 \(두 분류 사이 간격 큼\)
* C축으로 투영시 분별 가능 \(두 분류 사이 간격 작음\)
* \[결론\] C측이 분별력이 큼

#### C. 최적화된 투영 축 찾기

**Step 1. 최적화 목적 함수 정리**

$$
J(W) = \frac{부류간 퍼짐}{부류내 퍼짐} = \frac{\mid \bar m_1 - \bar m_2 \mid }{\bar s_1^2 +\bar s_2^2}
$$

| ![](http://i.imgur.com/dyjp3Gf.png) | ![](http://i.imgur.com/tjgZIlH.png) |
| :--- | :--- |
| 행렬$$S_B$$를 부류간 퍼짐 행렬이라 부름 \(Between-class scatter matrix\) | 행렬$$S_W$$를 부류내 퍼짐 행렬이라 부름 \(Within-class scatter matrix\) |

$$
J(W) = \frac{W^TS_BW}{W^TS_WW}
$$

**Step 2. 목적함수 최적화**

$$
\frac{\partial J(W)}{\partial W} = 0 \Rightarrow (W^TS_WW)S_BW = (W^TS_BW)S_WW
$$

**Step 3. 식 정리 및 최종 식**

![](http://i.imgur.com/xLSMvzL.png)

> 상세 풀이 식 : 패턴인식, 오일석, 291P

Fisher의 선별 분별\(LD:Linear Discriminant\): $$W= \alpha S_W^{-1}(m_1 - m_2)$$

**\[예제 문제\]**

![](http://i.imgur.com/rW6Tody.png)

### 1.4  t-SNE

t-분포 확률적 임베딩\(t-SNE\)은 데이터의 차원 축소에 사용되는 기계 학습 알고리즘 중 하나로, 제프리 힌튼에 의해 개발되었다

> [In Raw Numpy: t-SNE](https://nlml.github.io/in-raw-numpy/in-raw-numpy-t-sne/)

## 2. 특징 선택\(Feature Selection

특징 선택 목적 : 분별력을 유지/향상 시키면서 특징 벡터의 차원을 줄이기

특징 선택은 d개의 특징을 갖는 특징 집합 F에서 $$\hat d$$개를 갖는 최적의 특징 부분 집합\(Feature subset\) $$\hat F$$ 를 선택 하는 문제

### 2.1 특징의 분별력

분별력\(Discriminatory power\) : 특징 벡터 X가 얼마나 좋은지의 척도, 부류 분리 능력

![](http://i.imgur.com/5kIWdbA.png) **\[ 특징 벡터는 부류내 분산은 작을수록, 부류간 분산은 클수록 좋다. a&gt;b&gt;c \]**

### 2.2 분별력 측정 기준

#### A. 다이버젼스\(Divergence\)

> 다이버전스:서로 다르게 움직이는 현상

두 부류의 확률 분포 $$p(X \mid \omega_i)$$와 $$p(X \mid \omega_j)$$의 거리가 멀수록 X는 두 부류를 잘 분별해 주므로 좋은 특징 벡터 이다.

![](http://i.imgur.com/ofp3RdH.png) **\[b가 더 좋은 특징 벡터 이다.\]**

두 확률 분포간의 거리 측정 방법 : KL\(Kullback-Leibler\) 다이버전스

* 두개 부류 : $$d_{ij}=KL(p(X \mid\omega_i), p(X\mid\omega_j))+KL(p(X\mid\omega_j),p(X\mid\omega_i))$$
* M개 부류 : $$d=\sum_{i=1}^{M}\sum_{j=1}^{M}P(\omega_i)P(\omega_j)d_{ij}$$

단점/제약

* 확률 분포를 알고 있을 때만 적용 가능
* 확률 분포 추정을 할수는 있지만 그럴경우 `차원의 저주`의 문제를 가짐

#### B. 훈련 샘플의 거리

샘플을 가지고 직접 측정 가능 \(cf. KL 다이버전스비해 현실적\)

부류 $$\omega_i$$에 속한 샘플 집합을 $$X_i$$라 하고, $$X-i$$의 k번째 샘플을 $$X_i^k$$로 표기

* 두개 부류 : $$d_{ij}=\frac{1}{N_iN_j}\sum_{k=1}^{N_i}\sum_{m=1}^{N_j}dist(X_i^k,X_j^m)$$
  * $$N_i$$: $$X_i$$의 샘플 개수
  * $$dist(X_i^k,X_j^m)$$: 두점 간의 거리, `마할라노비스거리`또는 `유클리드거리`척도 사용
* M개 부류 : $$d= \sum_{i=1}^M\sum_{j=1}^M P(\omega_i)P(\omega_j)d_{ij}$$

### 2.3 특징 선택을 위한 탐색 알고리즘

Feature selection은 $$2^d$$개의 해로 정의되는 방대한 탐색 공간을 가진다. 이 탐색 공간을 효율적으로 뒤질 것인가가 중요한 문제 이다.

#### A. 전역 탐색\(Global Search\) 알고리즘

개요 : 전체 특징 공간을 탐색

특징

* 최적해를 보장 
* 비 효율적\(시간안에 끝나지 않을 가능성 있음\)

종류

* 낱낱 탐색\(Exhaustive Search\) 알고리즘 : 기본 전역 탐색 알고리즘, 가능성 여부 없이 모든 것 탐색 
* 한정 분기\(Btansh 알고리즘 : 가능성이 없다고 판단되는것은 배제 하고 탐색

#### B. 순차 탐색\(Sequential Search\) 알고리즘

개요

* 전체 특징 집합 F에서 임의의 특징 집한 $$f_A$$를 선택하고,
* 나머지 특징 중에서 하나씩 add / rem 수행
* 성능 변화 관찰

종류

* SFS\(Sequential forward search\) 알고리즘: $$f_A$$에서 하나씩 추가 하며 찾기 
* SBS\(Sequential backward search\) 알고리즘: F에서 하나씩 빼며 찾기 
* PTA\(plus-p-take-away-q\) 알고리즘: NULL에서 추가와 제거를 번갈아 가면서 찾기 \(고정된 크기의 특징들\)
* SFFS\(Sequential forward floating search\) 알고리즘 : $$f_A$$에서 추가와 제거를 번갈아 가면서 찾기 \(동적인 크기의 특징들\)
* SBFS\(Sequential backward floating search\) 알고리즘 : F에서 추가와 제거를 번갈아 가면서 찾기 \(동적인 크기의 특징들\)

성능 비교

* 전방 탐색 : SFS &lt; PTA\(p&gt;q\) &lt; SFFS 
* 후방 탐색 : SBS &lt; PTS\(p&lt;q\) &lt; SBFS

#### C. 통계적 탐색 알고리즘

* 시뮬레이티드 어닐링
* 유전 알고리즘

