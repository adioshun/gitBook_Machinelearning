# cbu03\_C\_기계학습\_앙상블

Ensemble Methods.

* Classifier Fusion
  * “late fusion” vs. “early fusion” [\[참고-DeepDrive\]](https://www.gitbook.com/book/adioshun/deep_drive/edit#/edit/master/introearly-late-deep-fusion.md?_k=d5mlin)
* Cross-validation as Ensemble Evaluation
* Boosting
  * AdaBoost
* Random Forests

## 2.2 개요

* 여러 분류 알고리즘 다 돌려보고 결과를 투표를 통해 선택 
  * 주어진 학습 데이터 집합에 대해서 여러 개의 서로 다른 분류기를 만들고, 
  * 이들 분류기의 판정 결과를 투표/가중치투표 방식으로 결합
* 데이터가 적거나 많거나 상관없이 동작, Cross Validation과 유사 

> Ensemble Methods are based around the hypothesis that an **aggregated decision** from **multiple experts** can be superior to a decision from a single system.

## 2.2 앙상블 분류기 방식

* 배깅 : 한번에 여러 분류기 생성, eg. RandomForest
* 부스팅: 순차적으로 여러 분류기 생성, eg.AdaBoost

### A. 배깅\(bagging, bootstrap aggregating\)

* 붓스트랩을 통해 여러 개의 학습 데이터 집합을 만들고, 
* 각 학습 데이터집합별로 분류기를 만들어, 이들이 투표나 가중치 투표를 하여 최종판정을 하는 기법

> 각기 다른 분류기가 각기 다른 부분의 데이터 사용, 알고리즘이 다르다\(다양성\)

![](http://i.imgur.com/vcJPBbe.png)

| 부트스트랩\(cf. 복원추출\) |
| :--- |
| - 주어진 학습 데이터 집합에서 `복원추출`하여,   - 다수의 학습 데이터 집합을 만들어내는 기법 |
| 목적 : 똑같은 데이터 반복 사용 문제점 해결 |
| 효과: Overfitting방지 |
| Bootstrap : 장화를 신다. 시작 단계 |

#### \[예\] 서브배깅\(Subbagging\)

* Subsample + bagging 
* 원본데이터의 1/2정도의 크기로 샘플링
* 결과는 전체를 샘플링 한것과 비슷 

#### \[예\] Random forest

* 분류기로 결정트리를 사용하는 배깅 기법
* 여러개의 트리를 만들어서 사용 

#### \[예\] Robust Bagging

* 최종 분류기 선택시 평균\(=이상치 영향 받음\)을 사용하지 않고 중앙 값을 사용하는 알고리즘 

### B. 부스팅\(boosting\)

* k개의 분류기를 순차적으로 만들어 가는 앙상블 분류기 생성 방법
* 분류 정확도에 따라 학습 데이터에 **가중치**를 변경해가면서 분류기 생성
  * 순차적으로 분류기 생성, cf. 배깅은 한번에 만듬 
* 예측 성능이 떨어지는 규칙\(weak/base algorithm\)들을 조합하여 좀더 높은 성능을 발휘 하는 방법 

![](http://i.imgur.com/WBotdrm.png)

#### \[예\] Stumping

* 스텀핑 = 나무를 자를 떄 남는 작은 부분 의미 
* 트리에 적용되는 그한 형태의 부스팅 알고리즘 

#### \[예\] AdaBoost

* 프론더, 샤피어 제안\(1996\)
* 이학습자가 얼마나 어려워했는지에 따라서 데이터에 가중치 부여 전 
* 분류 규칙을 순차적으로 생성, 생성시 이전 분류 규칙에서 얻은 관측값에서 샘플 데이터의 분포를 재조정 
  * 이전 분류에서 오분류 데이터에는 높은 가중치 부여, 가중치$$=\alpha = \frac{1-\epsilon}{\epsilon}$$
  * 이전 분류에서 정분류 데이터에는 낮은 가중치 부여 
  * 목적: 분류하기 힘든 데이터에 우선권 부여 

### C. Mixture Algorithm

* 학습을 통해 분류기를 합치는 알고리즘 
* 네트워크를 학습시키는 알반적인 방법 = EM 알고리즘 

### D. Stacking

* Stacking은 Meta learner의 컨셉으로 소개되었으며 여러 모델들을 결합하는 방법이다.
* bagging이나 boosting보다 덜 널리 사용된다.
* bagging과 boosting과는 달리 stacking은 일반적으로 서로 다른 타입의 모델들을 결합하는데 사용된다.

> 참고 [TRYING ML](https://flonelin.wordpress.com/2016/08/02/stacking%EC%9D%B4%EB%9E%80/)

