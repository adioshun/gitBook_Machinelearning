# KB12\_혼성모델\_앙상블

에러는 두가지로 구성된다 variance + bias. 오버피팅되면 variance가 발생하고, 언더피팅되면 bias가 발생한다. 데이터가 많다면 동시 해결이 가능하지만 제한적 데이터에서는 앙상블이 해결책이다.

* variance해결 : 배깅\(eg. Random Forest\)
* bias해결 : 부스팅\(eg. adboosting, xgBoosting\)
* variance + bias 해결 : 스태킹  

![](http://i.imgur.com/w2Ngw2u.png)

앙상블 분류\(Ensemble Methods.\)

* Classifier Fusion
  * “late fusion” vs. “early fusion” [\[참고-DeepDrive\]](https://www.gitbook.com/book/adioshun/deep_drive/edit#/edit/master/introearly-late-deep-fusion.md?_k=d5mlin)
* Cross-validation as Ensemble Evaluation
* Boosting
  * AdaBoost
* Random Forests

앙상블 개요

* 여러 분류 알고리즘 다 돌려보고 결과를 투표를 통해 선택 
  * 주어진 학습 데이터 집합에 대해서 여러 개의 서로 다른 분류기를 만들고, 
  * 이들 분류기의 판정 결과를 투표/가중치투표 방식으로 결합
* 데이터가 적거나 많거나 상관없이 동작, Cross Validation과 유사 

> Ensemble Methods are based around the hypothesis that an **aggregated decision** from **multiple experts** can be superior to a decision from a single system.

앙상블 분류기 방식

* 배깅 : 한번에 여러 분류기 생성, eg. RandomForest
* 부스팅: 순차적으로 여러 분류기 생성, eg.AdaBoost

혼성 모델

* 앙상블 : 같은 문제에 서로 다른 알고리즘 적용 하여 문제 해결 
* 연산공유 : 하나의 해에 2개 이상의 알고리즘이 개입 \(eg. 유전알고리즘 + 순차 탐색, 신경망 + 퍼지\)

혼성 모델을 사용하는 몇 가지 이유

* 나쁜 운을 피할 수 있다.
* 성능 향상을 꾀할 수 있다.
* 데이터 양에 따른 어려움을 극복할 수 있다.
* 데이터 질에 따른 어려움을 극복할 수 있다.
* 다중 센서 시스템에서 효과적이다.
* 결정 경계가 너무 복잡한 경우에 효과적일 수 있다.
* 점진 학습이 가능하다.

![](http://i.imgur.com/wSZt4LW.png)

## 1. 앙상블 생성

앙상블\(다중 분류기\) 생성

* 방법 1 : 훈련집합을 재 샘플링 \(eg. 배깅, 부스팅\) 
* 방법 2 : 서로 다른 분류 알고리즘을 사용하는 방식 

| 다양성을 가지는 분류기 생성이 초점 |
| :--- |


> 메타 휴리스틱 : 다른 알고리즘을 포함하고 있는 알고리즘

### 1.1 배깅\(bagging, bootstrap aggregating\)

* 붓스트랩을 통해 여러 개의 학습 데이터 집합을 만들고,
* 각 학습 데이터집합별로 분류기를 만들어, 이들이 투표나 가중치 투표를 하여 최종판정을 하는 기법

> 각기 다른 분류기가 각기 다른 부분의 데이터 사용, 알고리즘이 다르다\(다양성\)

![](http://i.imgur.com/vcJPBbe.png)

| 부트스트랩\(cf. 복원추출\) |
| :--- |
| - 주어진 학습 데이터 집합에서 `복원추출`하여,  - 다수의 학습 데이터 집합을 만들어내는 기법 |
| 목적 : 똑같은 데이터 반복 사용 문제점 해결 |
| 효과: Overfitting방지 |
| Bootstrap : 장화를 신다. 시작 단계 |

**\[예\] 서브배깅\(Subbagging\)**

* Subsample + bagging
* 원본데이터의 1/2정도의 크기로 샘플링
* 결과는 전체를 샘플링 한것과 비슷

**\[예\] Random forest**

* 분류기로 결정트리를 사용하는 배깅 기법
* 여러개의 트리를 만들어서 사용

**\[예\] Robust Bagging**

* 최종 분류기 선택시 평균\(=이상치 영향 받음\)을 사용하지 않고 중앙 값을 사용하는 알고리즘

### 1.2 부스팅\(boosting\)

* k개의 분류기를 순차적으로 만들어 가는 앙상블 분류기 생성 방법
* 분류 정확도에 따라 학습 데이터에 **가중치**를 변경해가면서 분류기 생성
* 순차적으로 분류기 생성, cf. 배깅은 한번에 만듬
* 예측 성능이 떨어지는 규칙\(weak/base algorithm\)들을 조합하여 좀더 높은 성능을 발휘 하는 방법

![](http://i.imgur.com/WBotdrm.png)

**\[예\] Stumping**

* 스텀핑 = 나무를 자를 떄 남는 작은 부분 의미
* 트리에 적용되는 그한 형태의 부스팅 알고리즘

**\[예\] AdaBoost**

* 프론더, 샤피어 제안\(1996\)
* 이학습자가 얼마나 어려워했는지에 따라서 데이터에 가중치 부여 전
* 분류 규칙을 순차적으로 생성, 생성시 이전 분류 규칙에서 얻은 관측값에서 샘플 데이터의 분포를 재조정
* 이전 분류에서 오분류 데이터에는 높은 가중치 부여, 가중치$$=\alpha = \frac{1-\epsilon}{\epsilon}$$
* 이전 분류에서 정분류 데이터에는 낮은 가중치 부여
* 목적: 분류하기 힘든 데이터에 우선권 부여

### 1.3 Mixture Algorithm

* 학습을 통해 분류기를 합치는 알고리즘
* 네트워크를 학습시키는 알반적인 방법 = EM 알고리즘

### 1.4 Stacking

* Stacking은 Meta learner의 컨셉으로 소개되었으며 여러 모델들을 결합하는 방법이다.
* bagging이나 boosting보다 덜 널리 사용된다.
* bagging과 boosting과는 달리 stacking은 일반적으로 서로 다른 타입의 모델들을 결합하는데 사용된다.

> 참고 [TRYING ML](https://flonelin.wordpress.com/2016/08/02/stacking%EC%9D%B4%EB%9E%80/)

## 2. 앙상블 결합

목적: 다중 분류기의 출력을 결합하여 하나의 분류 결과를 만드는 과정

고려 사항 : 요소 분류기의 출력 특성을 고려 하여야 함

* Class label\(부류 표지\) : 1 or 0
* Class ranking\(부류 순위\) : 1 ~ M 까지의 정수
* Class probability\(부류 확률\) : 부류에 속할 확률 \(eg. 베이시언 분류기\)

Class probability $$\rightarrow$$ Class label/ranking로 변환 가능 \(단, 반대는 불가\)

> 신경망, SVM은 부류별 실수값 출력\(확률아님\), Softmax\(\)로 변환하여 확률로 간주 가능

### 2.1 Class label\(부류 표지\)

#### A. 다수 투표 \(Majority voting\)

최다 투표자를 선출 하는 방식과 유사

$$
q = argmax_{j=1,M}\sum^T_{t=1} l_{tj}
$$

* 미지의 패턴 x를 T개의 분류기로 분류 하여 $$L_1, L_2, ..., L_T$$를 구하고 q를 결정하여 최종적으로 x를 $$\omega_q$$ 로 분류  

#### B. 가중 다수 투표 \(Weighted Majority voting\)

* 명성이 있는\(신뢰도가 높은\) 분류기에 큰 비중을 두는 방식 

  $$
  q = argmax_{j=1,M}\sum^T_{t=1} \alpha_tl_{tj}
  $$

#### C. 행위 지식 공간 \(BKS: Behavior Knowledge space\)

기존 방식은 유권자가 후보자 각각에 대해 알고 있는 지식을 고려 하지 않았다.

BKS = 사전지식을 고려 하는 방식 \(eg. 후보자 2는 특정 Task\_a에 대하여 정확도가 크다\)

### 2.2 Class Ranking\(부류 순위\)

유권자가 가장 좋아 하는 한명의 후보에게 투표하는 것이 아니라, M명의 후보를 선호도에 따라 순위\(Rank\)를 매기는 것과 유사

#### A. Borda 계수

1770년 Hean Charles de Borda가 제안. \(eg. 국회의원선출\(슬로베이아\), 야구 MVP선출\(한국\)

$$
q = argmax_{j=1,M}\sum^T_{t=1}S_{tj}
$$

### 2.3 Class Probability\(부류 확률\)

확률은 0~1 사이의 실수 이므로 아래와 같은 연산이 가능하다.

![](http://i.imgur.com/fKucNTe.png)

위 연산중의 하나로 $$\beta$$를 구하고 벡터 형태 B에서 가장 큰값을 같는 부류 $$\omega_q$$를 선택 하면 된다.

* 일반적으로 합/가중합 규칙을 많이 사용 

$$
q = argmax_{j=1,M}\beta_j
$$

## 3. 앙상블 선택

Option적 요소\(반드시 수행할 필요 없음\)

### 3.1 척도

선택을 위하여 다양한 척도들이 존재 한다.

[\[Measures of Diversity in Classifier Ensembles and Their Relationship with the Ensemble Accuracy\]](https://link.springer.com/article/10.1023%2FA%3A1022859003006?LI=true)에서 정리한 10개의 척도 중 7개를 중심적으로 살펴봄

#### A. 분류기 쌍간의 값을 활용 하여 다양성 판단

![](http://i.imgur.com/XituHy9.png) T개 분류기 간의 다양성 \(평균이용\)

* 모두 \[0,1\]사이의 값을 가짐
* 불일치를 빼고 값이 클수록 다양성 떨어짐 \(불일치는 반대\)

**가. Q-통계**

* 1900년 Yule제안한 척도 
* \[-1, 1\]사이의 값을 가짐 
  * 양수 : 두 분류기가 맞추는 경향이 어느정도 일지 
  * 음수 : 두 분류기가 틀리는 샘플이 많으면 

![](http://i.imgur.com/3TyQt3p.png)

**나. 상관계수**

* Q-통계와 같은 특성 가짐 

![](http://i.imgur.com/zl3Z4cA.png) $$q_ik=0$$일때 다양성이 최대가 된다.

**다. 불일치**

* 두 분류기의 의견이 같은 경우는 무시하고 다른 경우만 사용한다. 
* 불일치 척도는 \[0,1\] 사이의 값을 가짐 
  * 0 : 두 분류기가 모든 샘플에 대해 의견이 일치 
  * 1 : 두 분류기가 모든 샘플에 대해 의견이 불일치 

![](http://i.imgur.com/FwJ9uKE.png) $$d_ik=1$$일때 다양성이 최대가 된다.

**라. 이중과실**

* 두 분류기가 모두 과실을 범하는 경우만 사용 
* \[0,1\] 사이의 값을 가짐 
  * 0 : 다양성이 크다
  * 1 : 다양성이 낮다

![](http://i.imgur.com/S32LXIy.png) $$f_ik=1$$ 일때 다양성이 최소가 된다.

#### B. 바로 분류기의 값으로 다양성 판단

**가. 엔트로피**

* 기본 아이디어
  * 샘플을 맞추는 분류기와 틀리는 분류기가 5:5 라면 -&gt; 다양성이 크다. 
  * 모두 맞추거나, 모두 틀린다면 -&gt; 다양성이 작다 
* \[0,1\] 사이의 값을 가짐
  * 0 : 분류기들이 모두 같다
  * 1 : 다양성이 크다. 

![](http://i.imgur.com/prHeBp7.png)

**나. Kohavi-Wolpert 분산**

* \[0, $$\frac{1}{4}$$\]사이의 값을 가짐 
  * 0 : 분류기들이 모두 같음 
  * $$\frac{1}{4}$$ : 다양성이 가장 큰 경우 

![](http://i.imgur.com/NLYX4FQ.png)

**다. 평가자 동의\(Interrater agreement\)**

* p는 분류기들의 평균 정인식률
* 값이 클수록 다양성이 떨어 진다. 

![](http://i.imgur.com/8vES6VK.png)

### 3.2 선택 알고리즘

![](http://i.imgur.com/MwHxuXZ.png)

앙상블 선택 문제에 적용 가능한 알고리즘

* 임의 탐색 알고리즘 
* 개별 특징 평가 알고리즘 
* 낱낱 탐색 알고리즘 
* 한정 분기 알고리즘
* 순차 탐색 알고리즘
* 유전 알고리즘  
* [Bagging, Boosting and Stacking](http://hugrypiggykim.com/2019/04/07/bagging-boosting-and-stacking/)
* [앙상블](https://m.blog.naver.com/ydj9398/221403852142)
* agist adaboost\_algorithm : [PPT](https://www.slideshare.net/secret/HqstWsad2Mp5w?fbclid=IwAR1iyBiu8a8mAHAdYDKxvhPzrmDySDLVydb0mU3I9OlD3jmcgcMPTQWAN7M), [Youtube](https://www.youtube.com/watch?v=SLOlKnVLbPo&fbclid=IwAR0rnZUOxt-Fu9qm8V9rc7j4UfPWtwBMvyzn91D63WJ-f0v6ncuMGV73_yc) : A-GIST, 김양우, Ensemble, Random Forest \(Bagging\), Adaboost \(Boosting\)

